import argparse
import os
import random
import re
import sys
import time
from datetime import datetime
from pathlib import Path

import requests
from base_pyfile import make_directory, write_file
from dateutil.relativedelta import relativedelta

from bs4 import BeautifulSoup

# このファイルがあるディレクトリ
dir_base = Path(__file__).resolve().parent
episode = ""


def get_args():
    parser = argparse.ArgumentParser(description="小説家になろうの小説を取得するためのスクリプト")
    parser.add_argument("ncode", metavar="N", type=str, help="小説のNコード")
    parser.add_argument("--dir", type=str, default="./novel", help="小説の保存先ディレクトリ")
    args = parser.parse_args()
    return args


def main(ncode=None, resetFlag=False):
    if not ncode:
        args = get_args()
        ncode = args.ncode
        resetFlag = args.reset

    # ncodeのバリデーションチェック
    ncode = ncode.lower()
    if not re.match(r"n[0-9]{4}[a-z]{2}", ncode):
        print("Incorrect N-code!!")
        sys.exit(1)

    # 全部分数を取得
    info_url = f"https://ncode.syosetu.com/novelview/infotop/ncode/{ncode}/"
    info_url18 = f"https://novel18.syosetu.com/novelview/infotop/ncode/{ncode}/"

    # ユーザーエージェントの設定（設定必須）
    headers = {
        "User-Agent": f"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/{random.randint(500, 999)}.0 (KHTML, like Gecko) Chrome/{random.randint(80, 99)}.0.{random.randint(1000, 9999)}.87 Safari/{random.randint(500, 999)}.0"
    }
    # ここでcookieを指定
    cookie = {"over18": "yes"}  # クッキーの指定

    try:
        narou = True
        response = requests.get(url=info_url, headers=headers, cookies=cookie)
        info_res = response.content

    except Exception:
        narou = False
        try:
            # htmlの取得
            response = requests.get(url=info_url18, headers=headers, cookies=cookie)
            info_res = response.content
        except Exception:
            print("Incorrect N-code!!")
            sys.exit(1)

    soup = BeautifulSoup(info_res, "html.parser")
    pre_info = soup.select_one("#pre_info").text
    try:
        num_parts = int(re.search(r"全([0-9,]+)部分", pre_info).group(1).replace(",", ""))
    except:
        num_parts = 0
    # 小説を保存するディレクトリがなければ作成
    novel_dir = dir_base / ncode

    novel_dir.mkdir(parents=True, exist_ok=True)

    # すでに保存している部分番号のsetを取得
    re_part = re.compile(r"{}_([0-9]+).txt".format(ncode))
    existing_parts = {int(re_part.search(fn).group(1)) for fn in os.listdir(novel_dir)}

    # 新たに取得すべき部分番号のリストを生成
    # resetFlagがTrueならすべての部分を取得する
    if resetFlag:
        fetch_parts = list(range(1, num_parts + 1))
    else:
        fetch_parts = set(range(1, num_parts + 1)) - existing_parts
        fetch_parts = sorted(fetch_parts)

    num_fetch_rest = len(fetch_parts)
    for part in fetch_parts:
        # 作品本文ページのURL
        url = f"https://{'novel18' if narou else 'ncode'}.syosetu.com/{ncode}/"

        if num_parts:
            url += f"{part}/"

        response = requests.get(url=url, headers=headers, cookies=cookie)
        res = response.content

        soup = BeautifulSoup(res, "html.parser")

        title = soup.select_one("#title").text

        # CSSセレクタで本文を指定
        honbun = soup.select_one("#novel_honbun").text
        honbun += "\n"  # 次の部分との間は念のため改行しておく

        write_file(
            novel_dir / "epi" / f"{str(part).zfill(len(str(num_parts)))}_{title}.txt", honbun
        )
        write_file(novel_dir / f"{title}.txt", honbun)

        # 進捗を表示
        num_fetch_rest = num_fetch_rest - 1
        print("part {:d} downloaded (rest: {:d} parts)".format(part, num_fetch_rest))

        time.sleep(1)  # 次の部分取得までは1秒間の時間を空ける


def honbunsyutoku(url, file=Path("novel"), episodecount=0):
    global episode
    url = "https://kakuyomu.jp" + url
    episodecount += 1
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    write = soup.find("div", id="contentMain")
    # print(write)
    title = soup.find("title").text.split(" - ")[0]
    print(title)
    # id属性が"p"で始まるpタグを取得する
    p_tags = soup.find_all("p", id=lambda x: x and x.startswith("p"))

    p = "\n".join(str(tag.text) for tag in p_tags)
    print(p)
    write_file(str(file / "epi" / (episode.format(episodecount) + "_" + title)), p)
    write_file(file / title, p)
    time.sleep(2)

    try:
        next_episode_url = soup.find("a", id="contentMain-readNextEpisode")["href"]
        honbunsyutoku(next_episode_url, file, episodecount)
        time.sleep(1)
    except:
        pass


def Ranking_syousetu(url, novel_dir):
    url = "https://kakuyomu.jp" + url
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    episode_link = soup.select_one("li.widget-toc-episode a")["href"]

    honbunsyutoku(episode_link, novel_dir)


def get_kakuyomu_ranking():
    global episode
    url = "https://kakuyomu.jp/rankings/all/entire"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    for work in soup.select("div.float-left"):
        genre = work.select_one("span.widget-workCard-genre a").text
        novel = work.select_one("a.widget-workCard-titleLabel.bookWalker-work-title")
        url = novel["href"]
        episodeCount = int(
            work.select_one("span.widget-workCard-episodeCount").text[:-1].replace(",", "")
        )
        tags = " ".join(span.text for span in work.select("span.widget-workCard-tags span"))
        tag_all = f"{genre}　　{tags}"

        novel_title = novel.text
        novel_dir = Path("kakuyomu") / novel_title
        novel_dir.mkdir(parents=True, exist_ok=True)
        print(genre, url, episodeCount, tag_all, novel_title)
        write_file(novel_dir / genre, tag_all)

        episode = "{:0" + f"{len(str(episodeCount))}" + "}"

        Ranking_syousetu(url, novel_dir)


def add_months(date_string, months):
    date = datetime.strptime(date_string, "%Y%m%d")
    new_date = date + relativedelta(months=months)
    return new_date.strftime("%Y%m%d")



def get_narou_ranking():

    date = "20130501"
    
    while date < "20230402":

        date = add_months(date, 1)  # 1ヶ月増加


        url = r"https://api.syosetu.com/rank/rankget/?rtype=20130501-m"
        response = requests.get(url)
        print(response)


if __name__ == "__main__":
    # # main("n7648bn")
    # get_kakuyomu_ranking()
    
    get_narou_ranking()


    # headers = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0"}
